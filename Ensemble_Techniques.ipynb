{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDjNpun9mXiy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "b3rKWk9Fms90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems\n",
        "\n",
        "--->Yes, bagging can definitely be used for regression problems. Bagging, or Bootstrap Aggregating, is a general ensemble technique that works for both classification and regression because it simply trains multiple models on different bootstrap samples of the dataset and then combines their predictions. In regression, instead of taking a majority vote (as in classification), bagging averages the outputs of all the models to produce the final prediction. This averaging helps reduce variance, stabilize predictions, and prevent overfitting, especially when using high-variance models like decision trees. Algorithms like the Random Forest Regressor and Bagging Regressor in scikit-learn are common examples of bagging applied to regression tasks. Overall, bagging is effective for regression problems when the goal is to improve accuracy and robustness by combining multiple weak learners.\n"
      ],
      "metadata": {
        "id": "kubjjBDwmutV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between multiple model training and single model training\n",
        "\n",
        "--->\n",
        "\n",
        "\n",
        "**Single model training** builds and uses only one model on the entire dataset, so the final prediction comes from that single model alone. This approach is simple, fast, and easy to interpret, but it may suffer from high variance or overfitting if the model is unstable (like a deep decision tree). In contrast,\n",
        "\n",
        "**multiple model training** (used in ensemble methods like bagging, boosting, and stacking) trains several models on different versions or parts of the dataset and then combines their predictions through averaging, voting, or weighting. This improves accuracy, reduces variance, and makes the system more robust, but it requires more computation and is harder to interpret. In short, single models are simpler but less stable, while multiple model training is more powerful and reliable but computationally more expensive.\n"
      ],
      "metadata": {
        "id": "lJB1XkcKnBVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest\n",
        "\n",
        "--->**Feature randomness in Random Forest** means that the algorithm does **not** consider all features when splitting a node in a decision tree. Instead, at each split, it randomly selects a **subset of features** and chooses the best split only from within that subset. This randomness ensures that different trees in the forest learn different patterns from the data, making them less correlated with each other. As a result, the final ensemble becomes more robust and less likely to overfit compared to a single decision tree. In simple terms, feature randomness forces each tree to “look” at different parts of the data, increasing diversity among trees and improving the overall accuracy and stability of the Random Forest model.\n",
        "\n"
      ],
      "metadata": {
        "id": "jQtv1LH_nPIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score\n",
        "\n",
        "--->**OOB (Out-of-Bag) Score** is a built-in validation method used in Random Forests and other bagging-based models to estimate how well the model will perform on unseen data—without needing a separate test set.\n",
        "\n",
        "In bagging, each decision tree is trained on a bootstrap sample, which means some data points are randomly left out from that sample. These unused points are called **Out-of-Bag samples**. After a tree is trained, it is tested on its own OOB samples (the data it never saw during training). When you repeat this for all trees and aggregate the predictions, you get an overall **OOB score**, which acts like an internal cross-validation accuracy. It provides a reliable performance estimate, reduces the need for a separate validation dataset, and helps check for overfitting during training.\n"
      ],
      "metadata": {
        "id": "paW-cQg5naEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model\n",
        "\n",
        "--->\n",
        "\n",
        "\n",
        "\n",
        "### **1. Gini Importance (Mean Decrease in Impurity – MDI)**\n",
        "\n",
        "This method measures how much each feature reduces impurity (like Gini impurity or variance) whenever it is used to split a node.\n",
        "\n",
        "* Each time a feature is used, the model calculates how much the split improves purity.\n",
        "* These improvements are summed over all trees and normalized.\n",
        "* Features that consistently create better splits get higher importance scores.\n",
        "\n",
        "This method is fast and built-in, but it can sometimes favor features with more categories or continuous values.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Permutation Importance (Mean Decrease in Accuracy – MDA)**\n",
        "\n",
        "This method measures importance by checking how much the model’s performance drops when the values of a feature are randomly shuffled.\n",
        "\n",
        "* If shuffling a feature significantly reduces accuracy, the feature is important.\n",
        "* If accuracy stays almost the same, the feature contributes little to the model.\n",
        "\n",
        "This method is more reliable because it evaluates importance based on how the feature affects predictions, not just splits, but it is computationally more expensive.\n",
        "\n"
      ],
      "metadata": {
        "id": "rLgZ23-Pninn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the working principle of a Bagging Classifier\n",
        "\n",
        "-->The **Bagging Classifier** works by creating multiple versions of the same base model and training each one on a different random subset of the training data, generated through **bootstrap sampling** (sampling with replacement). Because each model sees a slightly different dataset, they learn different patterns. After all models are trained, the Bagging Classifier makes predictions by combining the outputs of all individual models—typically through **majority voting** for classification. This aggregation reduces variance, stabilizes predictions, and prevents overfitting, especially when using high-variance models like decision trees. In simple terms, bagging works by training many slightly different models and then letting them “vote” to produce a more accurate and robust final prediction.\n"
      ],
      "metadata": {
        "id": "EAo28enMny_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier’s performance\n",
        "\n",
        "--->\n",
        "\n",
        "A Bagging Classifier’s performance is typically evaluated using standard classification metrics such as **accuracy**, **precision**, **recall**, **F1-score**, and **confusion matrix** on a test dataset. You can also use **cross-validation** to check how well the model generalizes across different splits of the data. Bagging provides an additional built-in evaluation method called the **OOB (Out-of-Bag) score**, which uses the samples not included in each bootstrap dataset to estimate performance without needing a separate validation set. Together, these evaluation methods help measure how accurately, reliably, and robustly the Bagging Classifier performs on unseen data.\n"
      ],
      "metadata": {
        "id": "BwR91m67n60S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work\n",
        "\n",
        "-->A **Bagging Regressor** works by training multiple regression models on different random subsets of the training data and then averaging their predictions to produce a final output. It uses **bootstrap sampling** (sampling with replacement) to create various training sets, so each model sees a slightly different version of the data. Because the models are trained independently and capture different patterns, their errors tend to cancel out when averaged. This reduces variance, improves stability, and prevents overfitting—especially when using high-variance models like decision trees. In simple terms, the Bagging Regressor builds many diverse models and combines their outputs by averaging, resulting in more accurate and robust regression predictions than a single model.\n"
      ],
      "metadata": {
        "id": "q0TEy8eLoFDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques\n",
        "\n",
        "--->The main advantage of ensemble techniques is that they **combine the strengths of multiple models** to produce predictions that are **more accurate, more stable, and more robust** than any single model alone. By aggregating different learners—whether through averaging, voting, or sequential corrections—ensemble methods reduce variance, minimize bias, handle noise better, and significantly lower the risk of overfitting. In short, ensembles work because many weak or diverse models working together can outperform even a powerful individual model.\n"
      ],
      "metadata": {
        "id": "dNlkHbIcoN52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is the main challenge of ensemble methods\n",
        "\n",
        "-->The main challenge of ensemble methods is their **increased complexity and computational cost**. Because they combine multiple models instead of using just one, they require more time, more memory, and more processing power for both training and prediction. This can make them harder to deploy, slower to run, and more difficult to interpret compared to simple models. Additionally, ensembles can sometimes be tricky to tune, as they involve many parameters related to the base learners and how the models are combined. In summary, while ensembles offer high accuracy and robustness, their biggest challenge is the added complexity and resource requirements.\n"
      ],
      "metadata": {
        "id": "h_LuSwCBoVXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques\n",
        "\n",
        "-->The key idea behind ensemble techniques is that **combining multiple models leads to better performance than relying on a single model**. Each individual model may make different mistakes or capture different aspects of the data, but when their predictions are combined—through averaging, voting, or sequential correction—their individual errors tend to cancel out, resulting in higher accuracy, lower variance, and more stable predictions. In essence, ensembles work on the principle of “**wisdom of the crowd**,” where many diverse models working together produce a more reliable, robust, and powerful final prediction than any single model could on its own.\n"
      ],
      "metadata": {
        "id": "U94iwnVXodtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier\n",
        "\n",
        "-->A **Random Forest Classifier** is an ensemble learning algorithm that builds a large number of decision trees and combines their predictions to perform classification. Each tree in the forest is trained on a different bootstrap sample of the data, and at every split, the algorithm randomly selects a subset of features to consider. This randomness in both data sampling and feature selection ensures that the trees are diverse and not overly correlated. When making a prediction, each tree “votes” for a class, and the majority vote becomes the final output. This approach improves accuracy, reduces overfitting, and increases stability compared to using a single decision tree. In simple terms, a Random Forest Classifier is a collection of many randomly built decision trees that work together to make more reliable and robust classification predictions.\n"
      ],
      "metadata": {
        "id": "xy0SgDCRomZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques\n",
        "\n",
        "-->\n",
        "\n",
        "### **1. Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "Bagging trains multiple models independently on different bootstrap samples of the dataset and combines their predictions by averaging (regression) or voting (classification).\n",
        "\n",
        "* **Goal:** Reduce variance and prevent overfitting\n",
        "* **Examples:** Random Forest, Bagging Classifier, Bagging Regressor\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Boosting**\n",
        "\n",
        "Boosting trains models sequentially, where each new model focuses on correcting the errors made by previous ones. The final output is a weighted combination of all models.\n",
        "\n",
        "* **Goal:** Reduce bias and improve accuracy\n",
        "* **Examples:** AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Stacking (Stacked Generalization)**\n",
        "\n",
        "Stacking combines predictions from multiple different models (not just copies of the same one). A meta-model is trained on the outputs of these base models to produce the final prediction.\n",
        "\n",
        "* **Goal:** Leverage the strengths of diverse algorithms\n",
        "* **Examples:** Models like Logistic Regression or Linear Regression used as meta-learners\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uG0fX-tmotnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is ensemble learning in machine learning\n",
        "\n",
        "--->**Ensemble learning** in machine learning is a technique that combines multiple models to make better predictions than any single model could achieve on its own. Instead of relying on one algorithm, ensemble methods train several models—either of the same type (like many decision trees) or different types (like trees, SVMs, and logistic regression)—and then merge their outputs through voting, averaging, or meta-learning. The idea is that each model captures different patterns or makes different errors, and when their predictions are aggregated, the overall performance becomes more accurate, stable, and robust. In simple terms, ensemble learning works on the “wisdom of the crowd,” where combining multiple weak or diverse learners leads to a stronger and more reliable final model.\n"
      ],
      "metadata": {
        "id": "z1aDqZx4o5eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.When should we avoid using ensemble methods\n",
        "\n",
        "-->\n",
        "\n",
        "Ensemble methods should be avoided when you need a model that is simple, fast, easy to interpret, and lightweight, because ensembles are computationally expensive, require more memory, and are harder to explain compared to single models like Logistic Regression or Decision Trees. They are also unnecessary when the dataset is very small, since training multiple models on limited data may lead to overfitting instead of improvement. Additionally, if a single model already performs extremely well, using ensembles can add unnecessary complexity without meaningful gains. In real-time environments where predictions must be made very quickly or on low-powered devices, ensemble methods can also be impractical due to their slower inference speed.\n"
      ],
      "metadata": {
        "id": "9H82v2IWpAdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting\n",
        "\n",
        "-->Bagging helps reduce overfitting by training multiple models on different random subsets of the data and then combining their predictions, which smooths out the noise and instability of individual models. Because each model is trained on a unique bootstrap sample, they learn slightly different patterns, and their mistakes are not correlated. When the predictions of all these models are averaged (for regression) or voted on (for classification), the random errors made by individual models cancel out. This reduces the overall variance of the model and makes the final prediction more stable and less sensitive to noise in the training data. In simple terms, bagging prevents overfitting by reducing variance through the power of averaging many diverse models.\n"
      ],
      "metadata": {
        "id": "v1_i-XOspJ4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\n",
        "\n",
        "--->Random Forest is better than a single Decision Tree because it combines the predictions of many trees, making the final model far more accurate, stable, and robust. A single decision tree tends to overfit the training data—capturing noise and becoming highly sensitive to small changes in the dataset—because it has high variance. Random Forest reduces this variance by training multiple trees on different bootstrap samples and using random subsets of features at each split. These trees make diverse predictions, and when their outputs are aggregated through majority voting (classification) or averaging (regression), their individual errors cancel out. This leads to better generalization, improved accuracy, and much less overfitting compared to a single decision tree. In short, Random Forest works like a “team of trees,” and this collective decision-making makes it far stronger and more reliable than any single tree.\n"
      ],
      "metadata": {
        "id": "LYTlygjHpSRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the role of bootstrap sampling in Bagging\n",
        "\n",
        "--->The role of **bootstrap sampling** in Bagging is to create multiple diverse training datasets by randomly sampling the original dataset **with replacement**. Each bootstrap sample contains some repeated records and some omitted ones, resulting in slightly different versions of the dataset. These different samples ensure that each model in the ensemble learns unique patterns and makes different errors. This diversity among the models is crucial because when their predictions are combined through averaging or voting, the uncorrelated errors cancel out, reducing variance and preventing overfitting. In simple terms, bootstrap sampling gives each model a different perspective of the data, which makes the Bagging ensemble stronger, more stable, and more accurate.\n"
      ],
      "metadata": {
        "id": "4PwsyMfypa07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What are some real-world applications of ensemble techniques\n",
        "\n",
        "--->\n",
        "\n",
        "### **1. Finance**\n",
        "\n",
        "* Fraud detection\n",
        "* Credit risk scoring\n",
        "* Stock price prediction\n",
        "* Loan default prediction\n",
        "\n",
        "### **2. Healthcare**\n",
        "\n",
        "* Disease diagnosis (e.g., cancer detection)\n",
        "* Medical image classification\n",
        "* Predicting patient readmission or health risks\n",
        "\n",
        "### **3. E-commerce & Retail**\n",
        "\n",
        "* Recommendation systems\n",
        "* Customer churn prediction\n",
        "* Product demand forecasting\n",
        "\n",
        "### **4. Marketing**\n",
        "\n",
        "* Customer segmentation\n",
        "* Lead scoring\n",
        "* Predicting campaign success\n",
        "\n",
        "### **5. Cybersecurity**\n",
        "\n",
        "* Intrusion detection\n",
        "* Malware classification\n",
        "* Spam detection\n",
        "\n",
        "### **6. Natural Language Processing (NLP)**\n",
        "\n",
        "* Sentiment analysis\n",
        "* Document classification\n",
        "* Email filtering\n",
        "\n",
        "### **7. Computer Vision**\n",
        "\n",
        "* Image recognition\n",
        "* Object detection\n",
        "* Facial recognition systems\n",
        "\n",
        "### **8. Manufacturing**\n",
        "\n",
        "* Predictive maintenance\n",
        "* Quality control\n",
        "* Fault detection in machinery\n",
        "\n"
      ],
      "metadata": {
        "id": "RUT5xP7ipiYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What is the difference between Bagging and Boosting\n",
        "\n",
        "-->\n",
        "\n",
        "Bagging builds multiple models **independently** by training each one on a different bootstrap sample of the data, and then combines their predictions through averaging or majority voting to reduce variance and prevent overfitting. In contrast, Boosting builds models **sequentially**, where each new model focuses on correcting the errors of the previous ones, giving more weight to misclassified or hard-to-predict samples. This helps Boosting reduce bias and improve accuracy, but it can also make it more prone to overfitting and more sensitive to noise. In simple terms, **Bagging reduces variance by training models in parallel**, while **Boosting reduces bias by training models one after another, each improving on the last**.\n"
      ],
      "metadata": {
        "id": "lTO8VIY5punU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Practical answers***"
      ],
      "metadata": {
        "id": "CXB_PxEVp_by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "AHHpRkcIqK_u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b47eccf2",
        "outputId": "b6d20c25-3bc8-4636-d567-5a9124b070a5"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "PkRzuYoIqfhq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c04705a",
        "outputId": "c3c33b4f-79f5-46eb-e1f2-e86e741675f4"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "\n",
        "bagging_reg = BaggingRegressor(estimator=base_estimator_reg, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "bagging_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "\n",
        "y_pred_reg = bagging_reg.predict(X_test_reg)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "print(f\"Bagging Regressor Mean Squared Error (MSE): {mse:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Mean Squared Error (MSE): 211.5435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "WEPvkcPLqw61"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a4466b",
        "outputId": "e80b401f-f5e7-4432-ce3a-2a86b872db87"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "\n",
        "feature_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "\n",
        "sorted_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "print(\"Feature Importances (Random Forest Classifier):\")\n",
        "print(sorted_importances)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Random Forest Classifier):\n",
            "mean concave points        0.141934\n",
            "worst concave points       0.127136\n",
            "worst area                 0.118217\n",
            "mean concavity             0.080557\n",
            "worst radius               0.077975\n",
            "worst perimeter            0.074292\n",
            "mean perimeter             0.060092\n",
            "mean area                  0.053810\n",
            "worst concavity            0.041080\n",
            "mean radius                0.032312\n",
            "area error                 0.029538\n",
            "worst texture              0.018786\n",
            "worst compactness          0.017539\n",
            "radius error               0.016435\n",
            "worst symmetry             0.012929\n",
            "perimeter error            0.011770\n",
            "worst smoothness           0.011769\n",
            "mean texture               0.011064\n",
            "mean compactness           0.009216\n",
            "fractal dimension error    0.007135\n",
            "worst fractal dimension    0.006924\n",
            "mean smoothness            0.006223\n",
            "smoothness error           0.005881\n",
            "concavity error            0.005816\n",
            "compactness error          0.004596\n",
            "symmetry error             0.004001\n",
            "concave points error       0.003382\n",
            "mean symmetry              0.003278\n",
            "texture error              0.003172\n",
            "mean fractal dimension     0.003140\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "NvZSlU1DrCHg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d533583e",
        "outputId": "01a22265-b4c1-4a95-b6d5-9c1cec43505e"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "X_compare, y_compare = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(X_compare, y_compare, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_reg.fit(X_train_comp, y_train_comp)\n",
        "\n",
        "\n",
        "y_pred_rf = rf_reg.predict(X_test_comp)\n",
        "\n",
        "\n",
        "mse_rf = mean_squared_error(y_test_comp, y_pred_rf)\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "\n",
        "dt_reg.fit(X_train_comp, y_train_comp)\n",
        "\n",
        "\n",
        "y_pred_dt = dt_reg.predict(X_test_comp)\n",
        "\n",
        "\n",
        "mse_dt = mean_squared_error(y_test_comp, y_pred_dt)\n",
        "print(f\"Single Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "\n",
        "\n",
        "if mse_rf < mse_dt:\n",
        "    print(\"\\nRandom Forest Regressor performed better (lower MSE) than the single Decision Tree Regressor.\")\n",
        "elif mse_rf > mse_dt:\n",
        "    print(\"\\nSingle Decision Tree Regressor performed better (lower MSE) than the Random Forest Regressor.\")\n",
        "else:\n",
        "    print(\"\\nBoth models performed similarly.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor MSE: 209.3406\n",
            "Single Decision Tree Regressor MSE: 599.9199\n",
            "\n",
            "Random Forest Regressor performed better (lower MSE) than the single Decision Tree Regressor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "21-LtdayrWPg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f7c718b",
        "outputId": "b8729b89-4d5a-4fb4-891b-f4713a404495"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_oob, y_oob = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_oob, X_test_oob, y_train_oob, y_test_oob = train_test_split(X_oob, y_oob, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_oob_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "\n",
        "rf_oob_clf.fit(X_train_oob, y_train_oob)\n",
        "\n",
        "\n",
        "print(f\"Random Forest Classifier OOB Score: {rf_oob_clf.oob_score_:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier OOB Score: 0.9229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "l2U3s1-1rqjj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9b3acb",
        "outputId": "28d55193-c879-4fef-c726-4741e9b9b1d7"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_svm, y_svm = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator_svm = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_svm = BaggingClassifier(estimator=base_estimator_svm, n_estimators=10, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_svm.fit(X_train_svm, y_train_svm)\n",
        "\n",
        "\n",
        "y_pred_svm = bagging_clf_svm.predict(X_test_svm)\n",
        "\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy_svm:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier with SVM Accuracy: 0.7867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "oGhej_ccr4cU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc13433d",
        "outputId": "b65cab07-0edb-43c3-8d48-03870b443925"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_trees, y_trees = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_trees, X_test_trees, y_train_trees, y_test_trees = train_test_split(X_trees, y_trees, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "\n",
        "print(\"Comparing Random Forest Classifier accuracy with different numbers of trees:\\n\")\n",
        "\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "\n",
        "    rf_clf_ntrees = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "\n",
        "    rf_clf_ntrees.fit(X_train_trees, y_train_trees)\n",
        "\n",
        "\n",
        "    y_pred_ntrees = rf_clf_ntrees.predict(X_test_trees)\n",
        "\n",
        "\n",
        "    accuracy_ntrees = accuracy_score(y_test_trees, y_pred_ntrees)\n",
        "\n",
        "\n",
        "    print(f\"  Random Forest Classifier with {n_estimators} trees: Accuracy = {accuracy_ntrees:.4f}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Random Forest Classifier accuracy with different numbers of trees:\n",
            "\n",
            "  Random Forest Classifier with 10 trees: Accuracy = 0.9000\n",
            "  Random Forest Classifier with 50 trees: Accuracy = 0.9267\n",
            "  Random Forest Classifier with 100 trees: Accuracy = 0.9300\n",
            "  Random Forest Classifier with 200 trees: Accuracy = 0.9300\n",
            "  Random Forest Classifier with 500 trees: Accuracy = 0.9367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "Gr4rMKZIsKiZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5520302f",
        "outputId": "975c0ef6-32f0-4927-9fc4-466a8d46e590"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "X_lr, y_lr = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator_lr = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "\n",
        "bagging_clf_lr = BaggingClassifier(estimator=base_estimator_lr, n_estimators=10, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_lr.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "\n",
        "y_pred_proba_lr = bagging_clf_lr.predict_proba(X_test_lr)[:, 1]\n",
        "\n",
        "\n",
        "auc_score = roc_auc_score(y_test_lr, y_pred_proba_lr)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score:.4f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier with Logistic Regression AUC Score: 0.8415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "OkzYRnBgscnK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92c7009c",
        "outputId": "7d5f4964-f301-4cf5-a9a5-ca22d5264aa7"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "X_rf_imp, y_rf_imp = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_rf_imp, X_test_rf_imp, y_train_rf_imp, y_test_rf_imp = train_test_split(X_rf_imp, y_rf_imp, test_size=0.3, random_state=42)\n",
        "\n",
        ".\n",
        "rf_reg_imp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_reg_imp.fit(X_train_rf_imp, y_train_rf_imp)\n",
        "\n",
        "\n",
        "importances_rf = rf_reg_imp.feature_importances_\n",
        "\n",
        "\n",
        "feature_names_rf = [f'Feature_{i}' for i in range(X_rf_imp.shape[1])]\n",
        "\n",
        "\n",
        "feature_importances_df = pd.Series(importances_rf, index=feature_names_rf)\n",
        "\n",
        "sorted_importances_rf = feature_importances_df.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "print(\"Feature Importances (Random Forest Regressor):\")\n",
        "print(sorted_importances_rf)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Random Forest Regressor):\n",
            "Feature_9    0.743103\n",
            "Feature_7    0.203907\n",
            "Feature_1    0.014790\n",
            "Feature_8    0.011872\n",
            "Feature_2    0.007288\n",
            "Feature_0    0.004422\n",
            "Feature_6    0.004111\n",
            "Feature_4    0.003928\n",
            "Feature_5    0.003664\n",
            "Feature_3    0.002915\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy"
      ],
      "metadata": {
        "id": "jpo67qIUst7v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9537e4",
        "outputId": "f060dc3a-f7a1-4b31-b94a-d6fcaa56218a"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_ensemble, y_ensemble = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(X_ensemble, y_ensemble, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "bagging_base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "bagging_clf_ensemble = BaggingClassifier(estimator=bagging_base_estimator, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_ensemble.fit(X_train_ensemble, y_train_ensemble)\n",
        "\n",
        "\n",
        "y_pred_bagging = bagging_clf_ensemble.predict(X_test_ensemble)\n",
        "accuracy_bagging = accuracy_score(y_test_ensemble, y_pred_bagging)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rf_clf_ensemble = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_ensemble.fit(X_train_ensemble, y_train_ensemble)\n",
        "\n",
        "\n",
        "y_pred_rf = rf_clf_ensemble.predict(X_test_ensemble)\n",
        "accuracy_rf = accuracy_score(y_test_ensemble, y_pred_rf)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "if accuracy_rf > accuracy_bagging:\n",
        "    print(\"\\nRandom Forest Classifier performed better than Bagging Classifier.\")\n",
        "elif accuracy_bagging > accuracy_rf:\n",
        "    print(\"\\nBagging Classifier performed better than Random Forest Classifier.\")\n",
        "else:\n",
        "    print(\"\\nBoth Bagging and Random Forest Classifiers performed similarly.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9200\n",
            "Random Forest Classifier Accuracy: 0.9300\n",
            "\n",
            "Random Forest Classifier performed better than Bagging Classifier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "jV6PcYKjs87k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2df1343d",
        "outputId": "369e1e4b-c929-4117-b3f8-e7095135cf9a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_grid, y_grid = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_grid, X_test_grid, y_train_grid, y_test_grid = train_test_split(X_grid, y_grid, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_grid = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_leaf': [1, 5]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf_clf_grid, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "\n",
        "print(\"Starting GridSearchCV for hyperparameter tuning...\")\n",
        "grid_search.fit(X_train_grid, y_train_grid)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "\n",
        "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "best_rf_clf = grid_search.best_estimator_\n",
        "y_pred_best = best_rf_clf.predict(X_test_grid)\n",
        "accuracy_best = accuracy_score(y_test_grid, y_pred_best)\n",
        "print(f\"Test set accuracy with best parameters: {accuracy_best:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GridSearchCV for hyperparameter tuning...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "GridSearchCV complete.\n",
            "\n",
            "Best parameters found: {'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 50}\n",
            "Best cross-validation accuracy: 0.9215\n",
            "Test set accuracy with best parameters: 0.9267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "jPpSMAaAtMzj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "942c2f1f",
        "outputId": "7b6bde47-fbf1-4849-8a58-f5eabc98647e"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "X_bag_reg, y_bag_reg = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_bag_reg, X_test_bag_reg, y_train_bag_reg, y_test_bag_reg = train_test_split(X_bag_reg, y_bag_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "n_estimators_list_reg = [10, 50, 100, 200, 500]\n",
        "\n",
        "print(\"Comparing Bagging Regressor performance with different numbers of base estimators:\\n\")\n",
        "\n",
        "\n",
        "for n_estimators in n_estimators_list_reg:\n",
        "\n",
        "    base_estimator_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "\n",
        "    bagging_reg_ntrees = BaggingRegressor(estimator=base_estimator_reg, n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "\n",
        "    bagging_reg_ntrees.fit(X_train_bag_reg, y_train_bag_reg)\n",
        "\n",
        "\n",
        "    y_pred_bag_reg = bagging_reg_ntrees.predict(X_test_bag_reg)\n",
        "\n",
        "\n",
        "    mse_bag_reg = mean_squared_error(y_test_bag_reg, y_pred_bag_reg)\n",
        "\n",
        "\n",
        "    print(f\"  Bagging Regressor with {n_estimators} estimators: MSE = {mse_bag_reg:.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Bagging Regressor performance with different numbers of base estimators:\n",
            "\n",
            "  Bagging Regressor with 10 estimators: MSE = 249.6254\n",
            "  Bagging Regressor with 50 estimators: MSE = 218.6192\n",
            "  Bagging Regressor with 100 estimators: MSE = 211.5435\n",
            "  Bagging Regressor with 200 estimators: MSE = 208.8188\n",
            "  Bagging Regressor with 500 estimators: MSE = 205.8921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "nQg-ktZsteI-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2118ebb6",
        "outputId": "9cb6bab2-1205-406c-e80e-22af036c0a98"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_mis, y_mis = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_mis, X_test_mis, y_train_mis, y_test_mis = train_test_split(X_mis, y_mis, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_mis = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_mis.fit(X_train_mis, y_train_mis)\n",
        "\n",
        "\n",
        "y_pred_mis = rf_clf_mis.predict(X_test_mis)\n",
        "\n",
        "\n",
        "accuracy_mis = accuracy_score(y_test_mis, y_pred_mis)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_mis:.4f}\\n\")\n",
        "\n",
        "\n",
        "misclassified_indices = np.where(y_pred_mis != y_test_mis)[0]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "\n",
        "if len(misclassified_indices) > 0:\n",
        "    print(\"\\nDetails of some misclassified samples (up to 10):\")\n",
        "    for i, idx in enumerate(misclassified_indices):\n",
        "        if i >= 10:\n",
        "            break\n",
        "        sample_features = X_test_mis[idx]\n",
        "        actual_label = y_test_mis[idx]\n",
        "        predicted_label = y_pred_mis[idx]\n",
        "        print(f\"Sample Index (in test set): {idx}\")\n",
        "        print(f\"  Actual Label: {actual_label}, Predicted Label: {predicted_label}\")\n",
        "        print(f\"  Features: {sample_features}\\n\")\n",
        "else:\n",
        "    print(\"No misclassified samples found in the test set.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier Accuracy: 0.9300\n",
            "\n",
            "Number of misclassified samples: 21\n",
            "\n",
            "Details of some misclassified samples (up to 10):\n",
            "Sample Index (in test set): 4\n",
            "  Actual Label: 0, Predicted Label: 1\n",
            "  Features: [ 1.0582201   0.17454306  1.53204131  0.30900693 -1.51493715  0.15773521\n",
            " -0.08471043  4.23576819  0.5016951  -0.26829818]\n",
            "\n",
            "Sample Index (in test set): 25\n",
            "  Actual Label: 1, Predicted Label: 0\n",
            "  Features: [-2.07804459 -0.46480878  1.51745182  1.41306084  2.38609075 -0.20449966\n",
            "  0.16928802 -0.27053411 -0.16558708 -0.92503065]\n",
            "\n",
            "Sample Index (in test set): 35\n",
            "  Actual Label: 0, Predicted Label: 1\n",
            "  Features: [ 1.68348286  1.06119428  1.4667239   1.19074612  0.12640208  0.70612855\n",
            " -0.67186741  0.54481642 -0.11890253  0.48332483]\n",
            "\n",
            "Sample Index (in test set): 40\n",
            "  Actual Label: 1, Predicted Label: 0\n",
            "  Features: [ 0.0718109  -0.5641148   0.01809239  1.5125307  -0.75017053 -0.13926871\n",
            "  0.6572011  -1.71818057  2.50850068 -1.13156261]\n",
            "\n",
            "Sample Index (in test set): 51\n",
            "  Actual Label: 1, Predicted Label: 0\n",
            "  Features: [-1.42804325  0.76010803 -1.13630575  0.29923646  1.01607717  0.76759008\n",
            "  0.30074902 -1.3508877   1.02346012  0.74244424]\n",
            "\n",
            "Sample Index (in test set): 59\n",
            "  Actual Label: 0, Predicted Label: 1\n",
            "  Features: [ 0.25646047 -0.10423798  1.09888759 -1.75805519  0.0645544  -0.6302828\n",
            "  0.28902205 -1.2951367  -0.55822368 -1.21918583]\n",
            "\n",
            "Sample Index (in test set): 69\n",
            "  Actual Label: 1, Predicted Label: 0\n",
            "  Features: [ 1.0162624  -0.48536704  2.07605741  1.16319352  1.05946122 -0.03065611\n",
            " -1.03770482 -0.09276374  0.86484075 -0.56582351]\n",
            "\n",
            "Sample Index (in test set): 70\n",
            "  Actual Label: 0, Predicted Label: 1\n",
            "  Features: [-0.5092912  -0.1887116   3.17021748 -0.54839914  2.14975374 -1.90761181\n",
            " -0.57935383  0.62696518 -0.95186597  1.09948703]\n",
            "\n",
            "Sample Index (in test set): 76\n",
            "  Actual Label: 1, Predicted Label: 0\n",
            "  Features: [ 3.07136999 -0.35476852 -0.36593414 -0.27888364  0.74352589  0.96923059\n",
            "  0.827708   -0.96219144  3.65697828  2.05314018]\n",
            "\n",
            "Sample Index (in test set): 78\n",
            "  Actual Label: 0, Predicted Label: 1\n",
            "  Features: [ 0.2470566   0.20441437  0.69860622  0.96904055 -2.18426046 -0.0079812\n",
            "  0.44725216  2.47004277  1.09874156 -1.70017601]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "_aWIo9I8ttCU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87171b23",
        "outputId": "fc4c7c73-c7e0-4250-ac87-c5b7b14032bb"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_compare_clf, y_compare_clf = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_compare_clf, X_test_compare_clf, y_train_compare_clf, y_test_compare_clf = train_test_split(X_compare_clf, y_compare_clf, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "bagging_base_estimator_clf = DecisionTreeClassifier(random_state=42)\n",
        "bagging_clf_compare = BaggingClassifier(estimator=bagging_base_estimator_clf, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_compare.fit(X_train_compare_clf, y_train_compare_clf)\n",
        "\n",
        "\n",
        "y_pred_bagging_compare = bagging_clf_compare.predict(X_test_compare_clf)\n",
        "accuracy_bagging_compare = accuracy_score(y_test_compare_clf, y_pred_bagging_compare)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging_compare:.4f}\")\n",
        "\n",
        "\n",
        "dt_clf_compare = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "dt_clf_compare.fit(X_train_compare_clf, y_train_compare_clf)\n",
        "\n",
        "y_pred_dt_compare = dt_clf_compare.predict(X_test_compare_clf)\n",
        "accuracy_dt_compare = accuracy_score(y_test_compare_clf, y_pred_dt_compare)\n",
        "print(f\"Single Decision Tree Classifier Accuracy: {accuracy_dt_compare:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "if accuracy_bagging_compare > accuracy_dt_compare:\n",
        "    print(\"\\nBagging Classifier performed better (higher accuracy) than the single Decision Tree Classifier.\")\n",
        "elif accuracy_dt_compare > accuracy_bagging_compare:\n",
        "    print(\"\\nSingle Decision Tree Classifier performed better (higher accuracy) than the Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"\\nBoth Bagging and single Decision Tree Classifiers performed similarly.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9200\n",
            "Single Decision Tree Classifier Accuracy: 0.8867\n",
            "\n",
            "Bagging Classifier performed better (higher accuracy) than the single Decision Tree Classifier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "130CRpHRt-pp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "f0a8f48a",
        "outputId": "c2163c28-d539-4fb0-b980-431398caaf6b"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "X_cm, y_cm = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_cm, X_test_cm, y_train_cm, y_test_cm = train_test_split(X_cm, y_cm, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_cm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf_cm.fit(X_train_cm, y_train_cm)\n",
        "\n",
        "y_pred_cm = rf_clf_cm.predict(X_test_cm)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test_cm, y_pred_cm)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=rf_clf_cm.classes_, yticklabels=rf_clf_cm.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "vT3HWbACuL8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "woOh5cZ9qBr0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6f8c73",
        "outputId": "1087954c-6afc-4c44-be2c-627e837e60c7"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Generate a sample classification dataset\n",
        "X_stack, y_stack = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train_stack, X_test_stack, y_train_stack, y_test_stack = train_test_split(X_stack, y_stack, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Define base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', probability=True, random_state=42)), # probability=True needed for predict_proba\n",
        "    ('lr', LogisticRegression(random_state=42, solver='liblinear'))\n",
        "]\n",
        "\n",
        "# 4. Define the meta-classifier\n",
        "# This model will combine the predictions of the base estimators.\n",
        "final_estimator = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# 5. Initialize the Stacking Classifier\n",
        "# cv: Number of cross-validation folds to train base estimators.\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5, # Using 5-fold cross-validation for training base estimators\n",
        "    stack_method='auto',\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "# 6. Train the Stacking Classifier\n",
        "print(\"Training Stacking Classifier...\")\n",
        "stack_clf.fit(X_train_stack, y_train_stack)\n",
        "print(\"Stacking Classifier training complete.\")\n",
        "\n",
        "# 7. Make predictions on the test set\n",
        "y_pred_stack = stack_clf.predict(X_test_stack)\n",
        "\n",
        "# 8. Calculate and print the accuracy\n",
        "accuracy_stack = accuracy_score(y_test_stack, y_pred_stack)\n",
        "print(f\"\\nStacking Classifier Accuracy: {accuracy_stack:.4f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking Classifier...\n",
            "Stacking Classifier training complete.\n",
            "\n",
            "Stacking Classifier Accuracy: 0.8800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "D64qkgjtvEyb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c9ef659",
        "outputId": "c198a55b-cb44-44dc-cdc1-9a00bf5e364a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "\n",
        "feature_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "\n",
        "sorted_importances = feature_importances.sort_values(ascending=False)\n",
        "top_5_important_features = sorted_importances.head(5)\n",
        "\n",
        "\n",
        "print(\"Top 5 Most Important Features (Random Forest Classifier):\")\n",
        "print(top_5_important_features)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features (Random Forest Classifier):\n",
            "mean concave points     0.141934\n",
            "worst concave points    0.127136\n",
            "worst area              0.118217\n",
            "mean concavity          0.080557\n",
            "worst radius            0.077975\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "wljMElKsvVid"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd56c43",
        "outputId": "1974aa9c-b296-4a36-952b-1e0cf523c9b4"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "\n",
        "X_metrics, y_metrics = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_metrics, X_test_metrics, y_train_metrics, y_test_metrics = train_test_split(X_metrics, y_metrics, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator_metrics = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_metrics = BaggingClassifier(estimator=base_estimator_metrics, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_metrics.fit(X_train_metrics, y_train_metrics)\n",
        "\n",
        "\n",
        "y_pred_metrics = bagging_clf_metrics.predict(X_test_metrics)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test_metrics, y_pred_metrics)\n",
        "precision = precision_score(y_test_metrics, y_pred_metrics)\n",
        "recall = recall_score(y_test_metrics, y_pred_metrics)\n",
        "f1 = f1_score(y_test_metrics, y_pred_metrics)\n",
        "\n",
        "print(f\"Bagging Classifier Performance Metrics:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1-Score: {f1:.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Performance Metrics:\n",
            "  Accuracy: 0.9200\n",
            "  Precision: 0.8919\n",
            "  Recall: 0.9429\n",
            "  F1-Score: 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "V4IdPMwBvlPr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "223954c4",
        "outputId": "e075d05f-a6cc-4976-9e91-48566ec36586"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_depth, y_depth = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_depth, X_test_depth, y_train_depth, y_test_depth = train_test_split(X_depth, y_depth, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "max_depth_list = [None, 2, 5, 10, 20]\n",
        "\n",
        "print(\"Comparing Random Forest Classifier accuracy with different max_depth values:\\n\")\n",
        "\n",
        "\n",
        "for depth in max_depth_list:\n",
        "\n",
        "    rf_clf_depth = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "\n",
        "\n",
        "    rf_clf_depth.fit(X_train_depth, y_train_depth)\n",
        "\n",
        "\n",
        "    y_pred_depth = rf_clf_depth.predict(X_test_depth)\n",
        "\n",
        "\n",
        "    accuracy_depth = accuracy_score(y_test_depth, y_pred_depth)\n",
        "\n",
        "\n",
        "    print(f\"  Random Forest Classifier with max_depth={depth}: Accuracy = {accuracy_depth:.4f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Random Forest Classifier accuracy with different max_depth values:\n",
            "\n",
            "  Random Forest Classifier with max_depth=None: Accuracy = 0.9300\n",
            "  Random Forest Classifier with max_depth=2: Accuracy = 0.7500\n",
            "  Random Forest Classifier with max_depth=5: Accuracy = 0.9200\n",
            "  Random Forest Classifier with max_depth=10: Accuracy = 0.9267\n",
            "  Random Forest Classifier with max_depth=20: Accuracy = 0.9300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "Zkpp3T9tv2Dd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81e3dbef",
        "outputId": "6729e7ec-9fec-4cd5-874b-1e8eb7745418"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "X_reg_comp, y_reg_comp = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_reg_comp, X_test_reg_comp, y_train_reg_comp, y_test_reg_comp = train_test_split(X_reg_comp, y_reg_comp, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bg_dt_base = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "\n",
        "bagging_dt_reg = BaggingRegressor(estimator=bg_dt_base, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "\n",
        "\n",
        "print(\"Training Bagging Regressor with Decision Tree...\")\n",
        "bagging_dt_reg.fit(X_train_reg_comp, y_train_reg_comp)\n",
        "\n",
        "\n",
        "y_pred_dt_bag = bagging_dt_reg.predict(X_test_reg_comp)\n",
        "mse_dt_bag = mean_squared_error(y_test_reg_comp, y_pred_dt_bag)\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {mse_dt_bag:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bg_knn_base = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "\n",
        "bagging_knn_reg = BaggingRegressor(estimator=bg_knn_base, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "\n",
        "\n",
        "print(\"\\nTraining Bagging Regressor with K-Nearest Neighbors...\")\n",
        "bagging_knn_reg.fit(X_train_reg_comp, y_train_reg_comp)\n",
        "\n",
        "\n",
        "y_pred_knn_bag = bagging_knn_reg.predict(X_test_reg_comp)\n",
        "mse_knn_bag = mean_squared_error(y_test_reg_comp, y_pred_knn_bag)\n",
        "print(f\"Bagging Regressor (KNeighbors) MSE: {mse_knn_bag:.4f}\")\n",
        "\n",
        "\n",
        "if mse_dt_bag < mse_knn_bag:\n",
        "    print(\"\\nBagging Regressor with Decision Tree performed better (lower MSE).\")\n",
        "elif mse_knn_bag < mse_dt_bag:\n",
        "    print(\"\\nBagging Regressor with K-Nearest Neighbors performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"\\nBoth Bagging Regressors performed similarly.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bagging Regressor with Decision Tree...\n",
            "Bagging Regressor (Decision Tree) MSE: 218.6192\n",
            "\n",
            "Training Bagging Regressor with K-Nearest Neighbors...\n",
            "Bagging Regressor (KNeighbors) MSE: 906.0132\n",
            "\n",
            "Bagging Regressor with Decision Tree performed better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "Uk95toNGwNoM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f622cf69",
        "outputId": "96c8b63b-f904-404c-f7ee-c7726b086e56"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "X_roc, y_roc = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_roc, X_test_roc, y_train_roc, y_test_roc = train_test_split(X_roc, y_roc, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_roc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_roc.fit(X_train_roc, y_train_roc)\n",
        "\n",
        "\n",
        "y_pred_proba_roc = rf_clf_roc.predict_proba(X_test_roc)[:, 1]\n",
        "\n",
        "\n",
        "roc_auc = roc_auc_score(y_test_roc, y_pred_proba_roc)\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier ROC-AUC Score: 0.9609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "1wGvD5yCwbOr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd63406d",
        "outputId": "d743b557-b43f-45f2-80d2-bee820690dc8"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_cv, y_cv = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator_cv = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "bagging_clf_cv = BaggingClassifier(estimator=base_estimator_cv, n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "cv_scores = cross_val_score(bagging_clf_cv, X_cv, y_cv, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "\n",
        "print(f\"Bagging Classifier Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Average Cross-validation accuracy: {np.mean(cv_scores):.4f}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Cross-validation scores: [0.93  0.92  0.93  0.91  0.885]\n",
            "Average Cross-validation accuracy: 0.9150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "kV2Q15vTwo8H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "b9988d1b",
        "outputId": "982fe9f7-d27f-4e1e-dab8-9daafb123392"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X_pr, y_pr = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_pr, X_test_pr, y_train_pr, y_test_pr = train_test_split(X_pr, y_pr, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_pr = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "rf_clf_pr.fit(X_train_pr, y_train_pr)\n",
        "\n",
        "\n",
        "y_scores_pr = rf_clf_pr.predict_proba(X_test_pr)[:, 1]\n",
        "\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test_pr, y_scores_pr)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title('Precision-Recall Curve for Random Forest Classifier')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "sNYncSfaw6Rp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0000d41f",
        "outputId": "9ce61eac-ce79-4091-9fdd-2d144dd0b3eb"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_stack_rf_lr, y_stack_rf_lr = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
        "\n",
        "\n",
        "X_train_stack_rf_lr, X_test_stack_rf_lr, y_train_stack_rf_lr, y_test_stack_rf_lr = train_test_split(X_stack_rf_lr, y_stack_rf_lr, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "estimators_rf_lr = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr_base', LogisticRegression(random_state=42, solver='liblinear'))\n",
        "]\n",
        "\n",
        "\n",
        "final_estimator_rf_lr = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "\n",
        "stack_clf_rf_lr = StackingClassifier(\n",
        "    estimators=estimators_rf_lr,\n",
        "    final_estimator=final_estimator_rf_lr,\n",
        "    cv=5,\n",
        "    stack_method='auto',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Training Stacking Classifier with Random Forest and Logistic Regression...\")\n",
        "stack_clf_rf_lr.fit(X_train_stack_rf_lr, y_train_stack_rf_lr)\n",
        "print(\"Stacking Classifier training complete.\")\n",
        "\n",
        "\n",
        "y_pred_stack_rf_lr = stack_clf_rf_lr.predict(X_test_stack_rf_lr)\n",
        "\n",
        "accuracy_stack_rf_lr = accuracy_score(y_test_stack_rf_lr, y_pred_stack_rf_lr)\n",
        "print(f\"\\nStacking Classifier Accuracy (RF + LR): {accuracy_stack_rf_lr:.4f}\")\n",
        "\n",
        "\n",
        "rf_alone = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_alone.fit(X_train_stack_rf_lr, y_train_stack_rf_lr)\n",
        "y_pred_rf_alone = rf_alone.predict(X_test_stack_rf_lr)\n",
        "accuracy_rf_alone = accuracy_score(y_test_stack_rf_lr, y_pred_rf_alone)\n",
        "print(f\"Random Forest Classifier Alone Accuracy: {accuracy_rf_alone:.4f}\")\n",
        "\n",
        "\n",
        "lr_alone = LogisticRegression(random_state=42, solver='liblinear')\n",
        "lr_alone.fit(X_train_stack_rf_lr, y_train_stack_rf_lr)\n",
        "y_pred_lr_alone = lr_alone.predict(X_test_stack_rf_lr)\n",
        "accuracy_lr_alone = accuracy_score(y_test_stack_rf_lr, y_pred_lr_alone)\n",
        "print(f\"Logistic Regression Classifier Alone Accuracy: {accuracy_lr_alone:.4f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking Classifier with Random Forest and Logistic Regression...\n",
            "Stacking Classifier training complete.\n",
            "\n",
            "Stacking Classifier Accuracy (RF + LR): 0.9333\n",
            "Random Forest Classifier Alone Accuracy: 0.9300\n",
            "Logistic Regression Classifier Alone Accuracy: 0.7700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ],
      "metadata": {
        "id": "_uUE23MRxMzJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84689cad",
        "outputId": "159b4ca9-9e42-4148-e266-955d71771aad"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "X_bag_samples, y_bag_samples = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_bag_samples, X_test_bag_samples, y_train_bag_samples, y_test_bag_samples = train_test_split(X_bag_samples, y_bag_samples, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "max_samples_list = [0.5, 0.7, 0.9, 1.0]\n",
        "\n",
        "print(\"Comparing Bagging Regressor performance with different levels of bootstrap samples:\\n\")\n",
        "\n",
        "\n",
        "for max_samples_ratio in max_samples_list:\n",
        "\n",
        "    base_estimator_bag_samples = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "\n",
        "    bagging_reg_samples = BaggingRegressor(\n",
        "        estimator=base_estimator_bag_samples,\n",
        "        n_estimators=100,\n",
        "        max_samples=max_samples_ratio,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "\n",
        "    bagging_reg_samples.fit(X_train_bag_samples, y_train_bag_samples)\n",
        "\n",
        "\n",
        "    y_pred_bag_samples = bagging_reg_samples.predict(X_test_bag_samples)\n",
        "\n",
        "\n",
        "    mse_bag_samples = mean_squared_error(y_test_bag_samples, y_pred_bag_samples)\n",
        "\n",
        "\n",
        "    print(f\"  Bagging Regressor with max_samples={max_samples_ratio*100:.0f}%: MSE = {mse_bag_samples:.4f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Bagging Regressor performance with different levels of bootstrap samples:\n",
            "\n",
            "  Bagging Regressor with max_samples=50%: MSE = 237.3290\n",
            "  Bagging Regressor with max_samples=70%: MSE = 219.0417\n",
            "  Bagging Regressor with max_samples=90%: MSE = 212.4956\n",
            "  Bagging Regressor with max_samples=100%: MSE = 211.5435\n"
          ]
        }
      ]
    }
  ]
}